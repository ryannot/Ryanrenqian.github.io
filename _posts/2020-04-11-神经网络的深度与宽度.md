---
title: 2020-04-11-神经网络的深度与宽度

tags: 
        - 神经网络基础学习
---
作者：言有三
链接：https://www.zhihu.com/question/291790340/answer/667260880
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



首先看网络深度深度学习模型之所以在各种任务中取得了成功，足够的网络深度起到了很关键的作用，那么是不是模型越深，性能就越好呢？可以从以下几个方向来进行分析。**1 为什么加深可以提升性能**Bengio和LeCun在2017年的文章[1]中有这么一句话，"We claim that most functions that can be represented compactly by deep architectures cannot be represented by a compact shallow architecture"，大概意思就是大多数函数如果用一个深层结构刚刚好解决问题，那么就不可能用一个更浅的同样紧凑的结构来解决。要解决比较复杂的问题，要么增加深度，要么增加宽度，而增加宽度的代价往往远高于深度。Ronen Eldan等人甚至设计了一个能被小的3层网络表示，而不能被任意的2层网络表示的函数。总之，一定的深度是必要的。那么随着模型的加深，到底有哪些好处呢？**1.1、更好拟合特征。**现在的深度学习网络结构的主要模块是卷积，池化，激活，这是一个标准的非线性变换模块。**更深的模型，意味着更好的非线性表达能力，可以学习更加复杂的变换，从而可以拟合更加复杂的特征输入。**看下面的一个对比图[2]，实线是一个只有一层，20个神经元的模型，虚线是一个2层，每一层10个神经元的模型。从图中可以看出，2层的网络有更好的拟合能力，这个特性也适用于更深的网络。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir57e7jj30k005sglo.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7iri8b5mj30u008ogme.jpg)1.2、**网络更深，每一层要做的事情也更加简单了。**每一个网络层各司其职，我们从zfnet反卷积看一个经典的网络各个网络层学习到的权重。第一层学习到了边缘，第二层学习到了简单的形状，第三层开始学习到了目标的形状，更深的网络层能学习到更加复杂的表达。如果只有一层，那就意味着要学习的变换非常的复杂，这很难做到。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir1gtzmj30k00po0uu.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir394qij30ro0zitf2.jpg)上面就是网络加深带来的两个主要好处，**更强大的表达能力和逐层的特征学习**。**2 如何定量评估深度与模型性能**理论上一个2层的网络可以拟合任何有界的连续函数，但是需要的宽度很大，这在实际使用中不现实，因此我们才会使用深层网络。我们知道一个模型越深越好，但是怎么用一个指标直接定量衡量模型的能力和深度之间的关系，就有了**直接法和间接法两种方案**。**直接法便是定义指标理论分析网络的能力，间接法便是通过在任务中的一系列指标比如准确率等来进行比较。****2.1、直接法**早期对浅层网络的研究，通过研究函数的逼近能力，和布尔电路的比较，网络的VC维度等进行评估，但是并不适用于深层网络。目前直接评估网络性能一个比较好的研究思路是线性区间(linear regions)。可以将神经网络的表达看作是一个分段线性函数，如果要完美的拟合一个曲线，就需要无数多的线性区间(linear regions)。线性区间越多，说明网络越灵活。![img](https://pic4.zhimg.com/50/v2-2f16aabdf6ee2c375a584e216b6b5b87_hd.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir8d3o4j30u00l2aaw.jpg)Yoshua Bengio等人就通过**线性区间的数量来衡量模型的灵活性。一个更深的网络，可以将输入空间分为更多的线性响应空间，它的能力是浅层网络的指数级倍。**对于一个拥有n0个输入，n个输出，kn个隐藏层的单层网络，其最大数量为：![img](https://pic3.zhimg.com/50/v2-c21eb186be71362e4a63125f5d89aa5e_hd.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irekanaj30ba03awec.jpg)对于拥有同样多的参数，n0个输入，n个输出，k个隐藏层，每一层n个节点的多层网络，其最大数量为：![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir2wuf4j30eo06sdfq.jpg)![img](https://pic2.zhimg.com/80/v2-1e2cc6bf78ef66d73001661a7e99c2d1_1440w.jpg)因为n0通常很小，所以多层网络的数量是单层的指数倍(体现在k上)，计算方法是通过计算几何学来完成，大家可以参考论文[3]。除此之外还有一些其他的研究思路，比如monica binachini[4]等使用的betti number，Maithra Raghu等提出的trajectory length[5]。虽然在工程实践中这些指标没有多少意义甚至不一定有效，但是为我们理解深度和模型性能的关系提供了理论指导。**2.2、间接法**间接法就是展现实验结果了，网络的加深可以提升模型的性能，这几乎在所有的经典网络上都可以印证。比较不同的模型可能不够公平，那就从同一个系列的模型来再次感受一下，看看VGG系列模型，ResNet系列模型，结果都是从论文中获取。![img](https://pic2.zhimg.com/50/v2-ffb71cefd16048ec170199d7aa0044ec_hd.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir1g3tnj30pk09u0to.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir3p2alj30k008at8z.jpg)![img](https://pic1.zhimg.com/80/v2-91c9c9f1021cdc57f916223f07663fff_1440w.jpg)在一定的范围内，网络越深，性能的确越好。**3 加深就一定更好吗？**前面说到加深在一定程度上可以提升模型性能，但是未必就是网络越深越越好，我们从**性能提升和优化**两个方面来看。**3.1、加深带来的优化问题**ResNet为什么这么成功，就是因为它使得深层神经网络的训练成为可行。虽然好的初始化，BN层等技术也有助于更深层网络的训练，但是很少能突破30层。VGGNet19层，GoogleNet22层，MobileNet28层，经典的网络超过30层的也就是ResNet系列常见的ResNet50，ResNet152了。虽然这跟后面ImageNet比赛的落幕，大家开始追求更加高效实用的模型有关系，另一方面也是训练的问题。深层网络带来的**梯度不稳定，网络退化**的问题始终都是存在的，可以缓解，没法消除。这就有可能出现网络加深，性能反而开始下降。**3.2、网络加深带来的饱和**网络的深度不是越深越好，下面我们通过几个实验来证明就是了。公开论文中使用的ImageNet等数据集研究者已经做过很多实验了，我们另外选了两个数据集和两个模型。**第一个数据集是GHIM数据集，第二个数据集是从Place20中选择了20个类别，可见两者一个比较简单，一个比较困难。****第一个模型就是简单的卷积+激活的模型，第二个就是mobilenet模型。**首先我们看一下第一个模型的基准结构，包含5层卷积和一个全连接层， 因此我们称其为allconv6吧，表示深度为6的一个卷积网络。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir6x7ccj30hc0yiq3b.jpg)![img](https://pic3.zhimg.com/80/v2-843c5646b3288ee09d65badd26ce6d8e_1440w.jpg)接下来我们试验各种配置，从深度为5到深度为8，下面是每一个网络层的stride和通道数的配置。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irb6g50j30k00au750.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir45pbaj31400loado.jpg)我们看结果，优化都是采用了同一套参数配置，而且经过了调优，具体细节篇幅问题就不多说了。![img](https://pic2.zhimg.com/50/v2-125a9a9c6e92ae0a86cade1b65284c0b_hd.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irdhn7hj30u00g4dhj.jpg)看的出来网络加深性能并未下降，但是也没有多少提升了。allconv5的性能明显更差，深度肯定是其中的一个因素。我们还可以给所有的卷积层后添加BN层做个试验，结果如下，从allconv7_1和allconv8_1的性能相当且明显优于allconv6可以得出与刚才同样的结论。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irf1170j30k009ejrg.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir64xikj30u00e3gmb.jpg)那么，对于更加复杂的数据集，表现又是如何呢？下面看在place20上的结果，更加清晰了。![img](https://pic4.zhimg.com/50/v2-4c876275e0c66a27122a87786fa40fe3_hd.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irfuhd9j30u00mi41e.jpg)allconv5，allconv6结果明显比allconv7，allconv8差，而allconv7和allconv8性能相当。所以从allconv这个系列的网络结构来看，随着深度增加到allconv7，之后再简单增加深度就难以提升了。接下来我们再看一下**不同深度的mobilenet在这两个数据集上的表现**，原始的mobilenet是28层的结构。不同深度的MobileNet在GHIM数据集的结果如下：![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irdzqznj30k00avaad.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir5jrb4j30u00gamym.jpg)看得出来当模型到16层左右后，基本就饱和了。不同深度的MobileNet在Place20数据集的结果如下：![img](https://pic3.zhimg.com/50/v2-2eb5dc71761cba7f3716271e85039ea1_hd.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irapplkj30u00miq4p.jpg)与GHIM的结果相比，深度带来的提升更加明显一些，不过也渐趋饱和。这是必然存在的问题，哪有一直加深一直提升的道理，只是如何去把握这个深度，尚且无法定论，只能依靠更多的实验了。除此之外，模型加深还可能出现的一些问题是**导致某些浅层的学习能力下降**，限制了深层网络的学习，这也是跳层连接等结构能够发挥作用的很重要的因素。参考文献：[1] Bengio Y, LeCun Y. Scaling learning algorithms towards AI[J]. Large-scale kernel machines, 2007, 34(5): 1-41.[2] Montufar G F, Pascanu R, Cho K, et al. On the number of linear regions of deep neural networks[C]//Advances in neural information processing systems. 2014: 2924-2932.[3] Pascanu R, Montufar G, Bengio Y. On the number of response regions of deep feed forward networks with piece-wise linear activations[J]. arXiv preprint arXiv:1312.6098, 2013.[4] Bianchini M, Scarselli F. On the complexity of neural network classifiers: A comparison between shallow and deep architectures[J]. IEEE transactions on neural networks and learning systems, 2014, 25(8): 1553-1565.[5] Raghu M, Poole B, Kleinberg J, et al. On the expressive power of deep neural networks[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017: 2847-2854.**总结**深度学习的名字中就带着“深”，可见深度对模型的重要性。这一次我们讲述了深度对模型带来提升的原理，如何定量地评估深度对模型性能的贡献，以及加深网络会遇到的问题。
然后再来分析网络宽度。在一定的程度上，网络越深越宽，性能越好。**宽度，即通道(channel)的数量**。注意我们这里说的和宽度学习一类的模型没有关系，而是特指深度卷积神经网络的宽度。**1 为什么需要足够的宽度**网络更深带来的一个非常大的好处，就是逐层的抽象，不断精炼提取知识，如下图第一层学习到了边缘，第二层学习到了简单的形状，第三层开始学习到了目标的形状，更深的网络层能学习到更加复杂的表达。如果只有一层，那就意味着要学习的变换非常的复杂，这很难做到。![img](https://pic4.zhimg.com/50/v2-5a3d2cdf4e51ba3f99c46143bb214ef3_hd.jpg)![img](https://pic4.zhimg.com/80/v2-5a3d2cdf4e51ba3f99c46143bb214ef3_1440w.jpg)**而宽度就起到了另外一个作用，那就是让每一层学习到更加丰富的特征，比如不同方向，不同频率的纹理特征。**下面是AlexNet模型的第一个卷积层的96个通道，尽管其中有一些形状和纹理相似的卷积核(这将成为优化宽度的关键)，还是可以看到各种各种的模式。![img](https://pic3.zhimg.com/50/v2-7b6b463fd8c108253f92fa6045d62f40_hd.jpg)![img](https://pic3.zhimg.com/80/v2-7b6b463fd8c108253f92fa6045d62f40_1440w.jpg)因为该卷积层的输入是RGB彩色图，所以这里就将其可视化为3通道的彩色图，每一个大小是11*11。**有的是彩色有的是灰色，说明有的侧重于提取纹理信息，有的侧重于提取颜色信息。**可以发现卷积核可视化之后和Gabor特征算子其实很像。Gabor特征算子就是使用一系列不同频率的Gabor滤波核与图像卷积，得到图像上的每个点和附近区域的频率分布。通常有8个方向，5个尺度。太窄的网络，每一层能捕获的模式有限，此时网络再深都不可能提取到足够的信息往下层传递。**2 网络到底需要多宽**那么一个网络是越宽越好吗？我们又该如何利用好宽度呢？**2.1、网络宽度的下限在哪？**就算一个网络越宽越好，我们也希望效率越高越好，因为宽度带来的计算量是成平方数增长的。我们知道对于一个模型来说，浅层的特征非常重要，因此网络浅层的宽度是一个非常敏感的系数，那么发展了这么久，那些经典的网络第一个卷积层的宽度都是多少呢？![img](https://pic2.zhimg.com/50/v2-0ee0221afc4e36f23f3617b14f02d3aa_hd.jpg)![img](https://pic2.zhimg.com/80/v2-0ee0221afc4e36f23f3617b14f02d3aa_1440w.jpg)从AlexNet的96层到Vgg，Resnet等多数网络使用的64层，到高效网络Mobilenet的32层和Shufflenet的24层，似乎已经探到了下限，再往下性能就无法通过其他的方法来弥补了。前次我们说过有许多的研究都验证了网络必须具有足够的深度才能逼近一些函数，比如文[1]中构造的3层网络，如果想要2层网络能够逼近表达能力，宽度会是指数级的增加。**那么反过来，是不是也有一些函数只有足够宽才能够表达呢？**针对网络宽度的研究虽不如网络深度多，但是也有学者做了相关研究。文[2]中就提出了任何Lebesgue-integrable函数，不能被一个宽度小于n的ReLU网络逼近，n是输入的维度，Lebesgue-integrable函数就是满足下面积分条件的函数。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irae4jtj30dm05q745.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irj71s9j30dm05qa9z.jpg)**不过与深度不同的是，这样的一些函数宽度减少后，用于补偿模型性能的深度不是呈指数级增长，而是多项式增长，这似乎反应了宽度并没有深度那么重要。**不过不管怎么样，当前研究者们都从理论上探索了宽度和深度的下限，表明宽度和深度是缺一不可的。**2.2、网络宽度对模型性能的影响**网络的宽度自然也不是越宽越好，下面我们看看网络的宽度带来的性能提升。我们看一下Mobilenet网络的结果，Mobilenet研究了网络的宽度对性能的影响，通过一个乘因子来对每一层的宽度进行缩放，它们试验了1, 0.75, 0.5和0.25共4个值。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irhb2ykj30k007ot92.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir1vj7kj30eo06sdfq.jpg)从上面结果可以看得出来，性能是持续下降的。那么，是不是网络越宽越好呢？下面我们还是通过几个实验来证明就是了。公开论文中使用的ImageNet等数据集研究者已经做过很多实验了，我们另外选了两个数据集和一个全卷积模型。第一个数据集是GHIM数据集，第二个数据集是从Place20中选择了20个类别，可见两者一个比较简单，一个比较困难。使用全卷积模型的基准结构，包含5层卷积和一个全连接层， 因此我们称其为allconv6吧，表示深度为6的一个卷积网络。![img](https://pic3.zhimg.com/50/v2-843c5646b3288ee09d65badd26ce6d8e_hd.jpg)![img](https://pic3.zhimg.com/80/v2-843c5646b3288ee09d65badd26ce6d8e_1440w.jpg)对这个网络的各个卷积层，我们也设置了不同的参数配置如下，每一个卷积层的stride都等于2。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir7ei6oj30k008t0t2.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ird1oezj30u00d7ta3.jpg)首先我们比较Allconv6_1，Allconv6_2，Allconv6_3，Allconv6_4这4个模型和基准模型的结果，它们是以Allconv6_1为基础的模型。**Allconv6_1是各个通道数为baseline的四分之一的网络**，而Allconv6_2，Allconv6_3，Allconv6_4分别是将Allconv6_1的第1，2层，第3，4层，第5层卷积通道数加倍的网络。在GHIM数据集上的收敛结果如下：![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irfd3glj30k00b5q3e.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir7yc6rj30q30ej0ua.jpg)从上图结果可以看出，基准模型allconv6的性能最好，allconv6_2，allconv6_3，allconv6_4的模型性能都超过allconv6_1，说明此时增加任何一个网络层的通道数都有益于模型性能的提升，而且性能仍旧未超过基准模型。然后我们再看allconv6_5，allconv6_6，allconv6_7，allconv6_8与基准模型的对比，**allconv6_5的各层的通道数只有baseline模型的一半**。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir9rawqj30k00but94.jpg)![img](https://pic3.zhimg.com/80/v2-624e57241f5396412344ebf8df78a0ad_1440w.jpg)从上图可以看出，模型的性能相差不大，这说明allconv6_5已经有足够好的宽度，再增加无益于性能的提升。这一点可以通过Place20上的实验结果进行证明，结果如下：![img](https://pic2.zhimg.com/50/v2-057cf65b97506b15585283fa5228f7c1_hd.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irbnh24j30pr0eujsc.jpg)**2.3、网络宽度和深度谁更加重要？**这个问题目前没有答案，两者都很重要，不过目前的研究是模型性能对深度更加敏感，而调整宽度更加有利于提升模型性能。Mobilenet的作者们将深层更窄的网络和浅层更宽的网络进行了对比，去掉了conv5_2到conv5_6这5层不改变分辨率的depth seperable卷积块，结果对比如下：![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irckkq6j30k005x3yo.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irgkkt8j30u008v3zg.jpg)**更窄的网络拥有了更少的参数和更好的性能，这似乎也验证了增加网络的深度比增加网络的宽度更有利于提升性能。**在Wide Resnet网络中，作者们在CIFAR10和CIFAR100上用参数只是稍微增加的一个16层的宽网络取得了比1000层的窄网络更好的性能，而且计算代价更低。在ImageNet上50层的宽Resnet在参数增加少量的基础上，也比相应的ResNet152层的性能更好。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir9e907j30k004ft8w.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irin9i8j30u006n756.jpg)另一方面，宽度相对于深度对GPU也更加友好，因为GPU是并行处理的，许多研究也表明加宽网络比加深网络也更加容易训练。**根据笔者的经验，我们应该优先调整网络的宽度。****3 如何更加有效地利用宽度？**从前面的结果我们可知，网络的宽度是非常关键的参数，它体现在两个方面：(1) 宽度对计算量的贡献非常大。(2)宽度对性能的影响非常大。我们的追求当然是越窄同时性能越高的网络，确实很贪婪，不过这是要实现的目标，可以从以下几个方向入手。**3.1、提高每一层通道的利用率**宽度既然这么重要，那么每一个通道就要好好利用起来，所以，第一个发力点，便是提高每一层的通道利用率。下面我们首先观察一下AlexNet网络的第一个卷积层。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irc79inj30k00je3zh.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irl3gusj30k40jimyw.jpg)看出来了吧，有些卷积核很相似，相互之间可以通过反转得到，比如前面两个，那么就只需要学习一个就行了，这便是网络参数互补现象，如果将减半后的通道补上它的反，会基本上相当于原有的模型。基于这个原理，文[3]便是通过输入通道取反和输入通道进行concat的方式来扩充通道。这样仅仅以原来一半的计算量便维持了原来的网络宽度和性能。**3.2、用其他通道的信息来补偿**这个思想在DenseNet网络中被发挥地淋漓尽致。DenseNet网络通过各层之间进行concat，可以在输入层保持非常小的通道数的配置下，实现高性能的网络。![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7irlhahbj30k00e9dg6.jpg)![img](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge7ir8tkapj30u00ldwfx.jpg)参考文献[1] Eldan R, Shamir O. The power of depth for feedforward neural networks[C]//Conference on learning theory. 2016: 907-940.[2] Lu Z, Pu H, Wang F, et al. The expressive power of neural networks: A view from the width[C]//Advances in Neural Information Processing Systems. 2017: 6231-6239.[3] Shang W, Sohn K, Almeida D, et al. Understanding and improving convolutional neural networks via concatenated rectified linear units[C]//international conference on machine learning. 2016: 2217-2225.[4] Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.**总结**深度学习成功的关键在于深，但是我们也不能忘了它的宽度，即通道数目，这对于模型性能的影响不亚于深度，在计算量上的影响甚至尤比深度更加重要。关于网络深度和宽度对模型性能的影响，先说这么多，更多可以去听我的知乎live。[如何设计性能更强的 CNN 网络结构？​www.zhihu.com![图标](https://pic4.zhimg.com/v2-41f5094bf991163a4166911db2806b1f_ipico.jpg)](https://www.zhihu.com/lives/1104173238430556160)
如果想增加对于深度学习相关理论的理解，可以查看我的《AI修行之路系列文章》，下面是其中第二个境界的文章。AI初识境系列完整阅读第一期：[【AI初识境】从3次人工智能潮起潮落说起](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031475%26idx%3D1%26sn%3D381e5ff44a9d724134d167aaab93393e%26chksm%3D8712bd4eb06534584d0f9dfe9840ca0a9afba5890c6935c63f2886b3a29adec0bc8ccef2ef6a%26scene%3D21%23wechat_redirect)第二期：[【AI初识境】从头理解神经网络-内行与外行的分水岭](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031503%26idx%3D1%26sn%3D52124c89fd52d197db4e3f089bceec3a%26chksm%3D8712bd32b0653424acdbdb1515ec009741bfe1a189eb44690cf71017ff0def71520534a4e5b3%26scene%3D21%23wechat_redirect)第三期：[【AI初识境】近20年深度学习在图像领域的重要进展节点](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031524%26idx%3D1%26sn%3D564750aea2c3c7cc03b6532852d1efe3%26chksm%3D8712bd19b065340f9fd87034bca58ec77a27ec75ef50accbcc807061135ddeff6ef34bdd55e0%26scene%3D21%23wechat_redirect)第四期：[【AI初识境】激活函数：从人工设计到自动搜索](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031541%26idx%3D1%26sn%3Db1fac1a1bce8cb27727ffea2b77b1689%26chksm%3D8712bd08b065341e0b4078dbd994f864dbd274571668968961881efb4a52ed0822c32a4742ba%26scene%3D21%23wechat_redirect)第五期：[【AI初识境】什么是深度学习成功的开始？参数初始化](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031561%26idx%3D1%26sn%3D8de2f0e398c1df0bdaebda99138dc22b%26chksm%3D8712bdf4b06534e2979cca8558f2817d4547676a768f3fc895dd578afda941999e48efd3cafb%26scene%3D21%23wechat_redirect)第六期：[【AI初识境】深度学习模型中的Normalization，你懂了多少？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031599%26idx%3D1%26sn%3Df06df4fe57024e7652ac6f6062253b32%26chksm%3D8712bdd2b06534c456f046d76f5f71696f294de6ce0f84736e0cea173eaa970c0a2d0015d72b%26scene%3D21%23wechat_redirect)第七期：[【AI初识境】为了围剿SGD大家这些年想过的那十几招](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031658%26idx%3D1%26sn%3Dfd1b54b24b607a9d28dc4e83ecc480fb%26chksm%3D8712bd97b065348132d8261907c56ce14077646dfc9c7531a4c3f1ecf6da1a488450428e4580%26scene%3D21%23wechat_redirect)第八期：[【AI初识境】被Hinton，DeepMind和斯坦福嫌弃的池化，到底是什么？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031740%26idx%3D1%26sn%3D2766cf718daf57a9c7f1556885cf35e9%26chksm%3D8712ba41b065335751aa0a50b6bbb1d6e230ed2f3d9a72914f1eb178ba0c2ecd9f77068fc0c0%26scene%3D21%23wechat_redirect)第九期：[【AI初识境】如何增加深度学习模型的泛化能力](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031822%26idx%3D1%26sn%3D2f5c0485ce54f9e1347bec48ee638072%26chksm%3D8712baf3b06533e5d89b949c3b5232665f428842f6712449785b20ba5dbc73ebf2a0f3f481e3%26scene%3D21%23wechat_redirect)第十期：[【AI初识境】深度学习模型评估，从图像分类到生成模型](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031923%26idx%3D1%26sn%3Dbcc3cef468f44d0a6de5b87ea00e5e5b%26chksm%3D8712ba8eb065339829ee84e7398e23d85dd7c4c7c154b96caead73c8815f887bb3c1bb7de063%26token%3D598159941%26lang%3Dzh_CN%23rd)第十一期：[【AI初识境】深度学习中常用的损失函数有哪些？](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032086%26idx%3D1%26sn%3Dfad93a8867bcc1c5b8e6b8db0260fe24%26chksm%3D8712bbebb06532fd8a1cd02df87db32ea17f07011405a00da844b160f88792b0581030e26565%26token%3D598159941%26lang%3Dzh_CN%23rd)第十二期：[【AI初识境】给深度学习新手开始项目时的10条建议](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D1%26sn%3D486dd16dec9a1df9b25aee23765e3f67%26chksm%3D8712bbb4b06532a21b8068e80c94be95b2148e3009abe816146ffc532a96a5aecd8e1dd9fcb0%26scene%3D21%23wechat_redirect)希望对大家有用。

[编辑于 2019-06-27]()

三人行，必有